{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorie1</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFORMATIQUE</td>\n",
       "      <td>batter acer aspir one ion mah noir compatibl b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TELEPHONIE - GPS</td>\n",
       "      <td>coqu rigid bleu lagon alcatel motif drapeau li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TELEPHONIE - GPS</td>\n",
       "      <td>facad coqu cellular lin shckgal minip marqu ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TELEPHONIE - GPS</td>\n",
       "      <td>coqu meteor tpu nexus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TELEPHONIE - GPS</td>\n",
       "      <td>coqu soupl transparent flex motif keep calm pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>DECO - LINGE - LUMINAIRE</td>\n",
       "      <td>souvenir franc tour eiffel miniatur prestig co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>LIBRAIRIE</td>\n",
       "      <td>fast track wast fre manufacturing john dav fas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>TELEPHONIE - GPS</td>\n",
       "      <td>fitbag bong giraf houss pochet telephon portab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>DECO - LINGE - LUMINAIRE</td>\n",
       "      <td>grand tableau minn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>TELEPHONIE - GPS</td>\n",
       "      <td>etui cuir emplac cart support samsung galaxy n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Categorie1  \\\n",
       "0                  INFORMATIQUE   \n",
       "1              TELEPHONIE - GPS   \n",
       "2              TELEPHONIE - GPS   \n",
       "3              TELEPHONIE - GPS   \n",
       "4              TELEPHONIE - GPS   \n",
       "...                         ...   \n",
       "99995  DECO - LINGE - LUMINAIRE   \n",
       "99996                 LIBRAIRIE   \n",
       "99997          TELEPHONIE - GPS   \n",
       "99998  DECO - LINGE - LUMINAIRE   \n",
       "99999          TELEPHONIE - GPS   \n",
       "\n",
       "                                             Description  \n",
       "0      batter acer aspir one ion mah noir compatibl b...  \n",
       "1      coqu rigid bleu lagon alcatel motif drapeau li...  \n",
       "2      facad coqu cellular lin shckgal minip marqu ag...  \n",
       "3                                 coqu meteor tpu nexus   \n",
       "4      coqu soupl transparent flex motif keep calm pl...  \n",
       "...                                                  ...  \n",
       "99995  souvenir franc tour eiffel miniatur prestig co...  \n",
       "99996  fast track wast fre manufacturing john dav fas...  \n",
       "99997  fitbag bong giraf houss pochet telephon portab...  \n",
       "99998                                grand tableau minn   \n",
       "99999  etui cuir emplac cart support samsung galaxy n...  \n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#le data frame est saved à partir de Help_rapport \n",
    "data_cleaned = pd.read_csv(\"data/data_cleaned_100k.csv\").fillna(\"\")\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['batter', 'acer', 'aspir', 'one', 'ion', 'mah', 'noir', 'compatibl', 'batter', ''], ['coqu', 'rigid', 'bleu', 'lagon', 'alcatel', 'motif', 'drapeau', 'liberi', 'film', 'coqu', 'rigid', 'ultra', 'fin', 'bleu', 'lagon', 'original', 'muzzano', 'motif', 'drapeau', 'liberi', 'alcatel', ''], ['facad', 'coqu', 'cellular', 'lin', 'shckgal', 'minip', 'marqu', 'agree', 'samsungmobil', 'compatibl', 'galaxy', 'minimatier', 'caoutchouc', 'soupl', ''], ['coqu', 'meteor', 'tpu', 'nexus', ''], ['coqu', 'soupl', 'transparent', 'flex', 'motif', 'keep', 'calm', 'play', 'football', 'coqu', 'soupl', 'ultra', 'fin', 'transparent', 'original', 'muzzano', 'motif', 'keep', 'calm', 'play', 'footbal', '']]\n"
     ]
    }
   ],
   "source": [
    "## On s'est rendu compte que le dernier token était \"\" pour chaque produit\n",
    "train_array_token = [line.split(' ') for line in data_cleaned.Description.values]\n",
    "print(train_array_token[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_dimension = 500\n",
    "hs = 0\n",
    "negative = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning skip-gram Word2Vec\n",
      "Learning time : 96.86 seconds Word2Vec\n"
     ]
    }
   ],
   "source": [
    "#augmenter min_count\n",
    "\n",
    "sg = 1\n",
    "print(\"Start learning skip-gram Word2Vec\")\n",
    "ts = time.time()\n",
    "model_sg = gensim.models.Word2Vec(train_array_token, sg=sg, hs=hs, negative=negative, min_count=1, size=Features_dimension)\n",
    "te = time.time()\n",
    "t_learning = te-ts\n",
    "print(\"Learning time : %.2f seconds Word2Vec\" %t_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utiliser une moyenne pondérée TFIDF (moyenne qui donne un poids à chq mot)\n",
    "#+le mot apparait de fois dans le document, - il aura d'importance\n",
    "\n",
    "#fonction qui crée la list de features\n",
    "\n",
    "def set_features(array_token_prod,dim_feature,model):\n",
    "    features= []\n",
    "    #on prend les tokens de chaque produit\n",
    "    for token_prod in array_token_prod:\n",
    "        #on crée le np final du produit\n",
    "        feat_prod= np.zeros(dim_feature)\n",
    "        # on prend chaque token dans la desc prod\n",
    "        for token in token_prod:\n",
    "            #on somme les vecteurs et on divise direct par le nombre de token\n",
    "            feat_prod +=model[token]/len(token_prod)\n",
    "            \n",
    "        #on rajoute le vectuer moyenné produit dans la list features\n",
    "        features += [feat_prod]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-52aca7a56739>:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feat_prod +=model[token]/len(token_prod)\n"
     ]
    }
   ],
   "source": [
    "#init notre list de features\n",
    "features=set_features(train_array_token,Features_dimension,model_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"features100k_word2vec.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(features, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien 100 000 vecteurs colonne de dim 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on va split comme précédemment avec ces nvx features et label\n",
    "label = data_cleaned[\"Categorie1\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,label, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procédure habituelle ensuite.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulbrunet/anaconda3/envs/Projet4A/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789.879961013794 sec\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "te = time.time()\n",
    "print(te-ts, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score apprentissage  0.9030055555555555\n",
      "Score test  0.90134\n"
     ]
    }
   ],
   "source": [
    "print(\"Score apprentissage \",lr.score(X_train,y_train))\n",
    "print(\"Score test \",lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autre modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = 0\n",
    "print(\"Start learning CBOW Word2Vec\")\n",
    "ts = time.time()\n",
    "model_cbow = gensim.models.Word2Vec(train_array_token, sg=sg, hs=hs, negative=negative, min_count=1, size=Features_dimension)\n",
    "te = time.time()\n",
    "t_learning = te-ts\n",
    "print(\"Learning time : %.2f seconds Word2Vec\" %t_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2=set_features(train_array_token,Features_dimension,model_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(features2,label, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "lr2 = LogisticRegression()\n",
    "lr2.fit(X_train2, y_train2)\n",
    "te = time.time()\n",
    "print(te-ts, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score apprentissage \",lr2.score(X_train2,y_train2))\n",
    "print(\"Score test \",lr2.score(X_test2,y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration utilisation modèle/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg.most_similar([\"montr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow.most_similar([\"montr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg[\"montr\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faire tourner sg sur le max de données avec données nettoyées\n",
    "#puis prediction avec la regression à comparer le mm modele avec les features de count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre-trained network\n",
    "\n",
    "\n",
    "Github du projet[https://github.com/Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors) appris sur 1Giga d'articles de wikipedia en mode **Skip-Gram*\n",
    "\n",
    "Vous pouvez télécharger ce modèle en suivant ce [lien](https://drive.google.com/file/d/0B0ZXk88koS2KM0pVTktxdG15TkE/view). Dezipez-le puis téléchargez le modèle en indiquant la direction du fichier \"fr/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_online_dir = \"data/fr/fr.bin\"\n",
    "#model_online_dir = \"ACOMPLETER/fr.bin\"\n",
    "model_online = gensim.models.Word2Vec.load(model_online_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
